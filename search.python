from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time

# Attach to an existing Chrome debugging session
chrome_options = Options()
chrome_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")

# Initialize WebDriver
driver = webdriver.Chrome(options=chrome_options)

try:
    print("Attaching to existing browser session...")

    # Navigate to the search page
    driver.get("https://aannet.org/search/newsearch.asp")
    time.sleep(5)

    # Debug: Print page source
    print("Page Source (for debugging):")
    print(driver.page_source)

    # Scrape data
    print("Starting to scrape data...")
    all_fellows = []
    total_pages = 42  # Update if necessary

    for page in range(1, total_pages + 1):
        print(f"Scraping page {page}...")

        try:
            # Wait for elements to load
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CLASS_NAME, "fellow-entry"))  # Update selector
            )

            # Scroll to the bottom to load dynamic content
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)

            # Extract fellow entries
            fellows = driver.find_elements(By.CLASS_NAME, "fellow-entry")  # Update selector
            print(f"Found {len(fellows)} entries on page {page}.")

            for fellow in fellows:
                try:
                    name = fellow.find_element(By.CLASS_NAME, "fellow-name").text  # Update selector
                    institution = fellow.find_element(By.CLASS_NAME, "fellow-institution").text  # Update selector
                    location = fellow.find_element(By.CLASS_NAME, "fellow-location").text  # Update selector
                    all_fellows.append([name, institution, location])
                except Exception as e:
                    print(f"Error parsing fellow entry: {e}")
                    continue
        except Exception as e:
            print(f"Error scraping page {page}: {e}")
            break

        # Navigate to the next page
        try:
            next_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.LINK_TEXT, str(page + 1)))
            )
            next_button.click()
            time.sleep(3)
        except Exception as e:
            print(f"Pagination issue on page {page}: {e}")
            print("No more pages or next button not found.")
            break

    # Save data to Excel
    print("Saving data to Excel...")
    df = pd.DataFrame(all_fellows, columns=["Name", "Institution", "Location"])
    df.to_excel("fellows_details.xlsx", index=False)
    print("Fellows' details saved to 'fellows_details.xlsx'.")

except Exception as e:
    print(f"An error occurred: {e}")

finally:
    driver.quit()
    print("Browser closed.")
